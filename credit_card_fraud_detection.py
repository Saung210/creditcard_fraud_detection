# -*- coding: utf-8 -*-
"""Copy of credit_card_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l7xDRnaW8MYPLFEvxtf5tOSYuTHQxWbU

**This dataset is from kaggle: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download**

Data Collection and Exploration
"""

import pandas as pd

data = pd.read_csv("/content/creditcard.csv")
print("Dataset's First 5 Rows\n")
print(data.head())
print("Data Information\n")
print(data.info())
print("Data Descriptive Statistics\n")
print(data.describe())

"""Data Cleaning and Preprocessing"""

from sklearn.preprocessing import StandardScaler #to standardize features by removing the mean and scaling to unit variance.

data['NormalizedAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))
data.drop(['Time', 'Amount'], axis=1, inplace=True)

print(data.head())

"""Exploratory Data Analysis (EDA)"""

#Visualizing the data
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize = (10,6))
sns.heatmap(data.corr(), annot=True, fmt=".2f")
plt.show()

"""Handling Class Imbalance"""

# Check for missing values in the entire dataset
print(data.isnull().sum())

#Dropping null values
data = data.dropna()
print(data.isnull().sum())

from imblearn.over_sampling import SMOTE
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load the dataset
data = pd.read_csv('creditcard.csv')

# Drop all rows containing null values
data = data.dropna()

# Normalize the 'Amount' feature
data['NormalizedAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))

# Drop original 'Amount' and 'Time' columns
data.drop(['Time', 'Amount'], axis=1, inplace=True)

# Separate features and target variable
X = data.drop('Class', axis=1)
y = data['Class']

# Apply SMOTE with a smaller k_neighbors value
sm = SMOTE(random_state=42, k_neighbors=1)
X_res, y_res = sm.fit_resample(X, y)

# Check the shape of the resampled datasets
print(f"Original dataset shape: {X.shape, y.shape}")
print(f"Resampled dataset shape: {X_res.shape, y_res.shape}")

"""Model Development"""

#Splitting the data into training and testing sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.3, random_state=42)

#Training multiple models and compare their performance
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, roc_auc_score

models = {
    'RandomForest' : RandomForestClassifier(n_estimators = 100, random_state=42),
    'LogisticRegression' : LogisticRegression(),
    'XGBoost' : XGBClassifier(use_label_encoder = False, eval_metric='logloss')
}

for name, model in models.items():
  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  print(f'{name} Classification Report: \n')
  print(classification_report(y_test, y_pred))
  print(f'{name} ROC-AUC: {roc_auc_score(y_test, y_pred)}\n')

from sklearn.metrics import confusion_matrix

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Extract TN, FP, FN, TP
TN = conf_matrix[0, 0]
FP = conf_matrix[0, 1]
FN = conf_matrix[1, 0]
TP = conf_matrix[1, 1]

print("True Negatives (TN):", TN)
print("False Positives (FP):", FP)
print("False Negatives (FN):", FN)
print("True Positives (TP):", TP)

#Visualizing confusion matrix
# Plot confusion matrix as a heatmap
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Non-Fraud', 'Predicted Fraud'],
            yticklabels=['Actual Non-Fraud', 'Actual Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

import numpy as np

cm = np.array(confusion_matrix(y_test, y_pred, labels=[1,0]))
confusion = pd.DataFrame(cm, index=['is Fraud', 'is Normal'],columns=['predicted fraud','predicted normal'])
confusion

"""Model Interpretation"""

!pip install shap
import shap

explainer = shap.TreeExplainer(models['RandomForest'])
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test)

"""Model Deployment"""

#Saving the trained model using joblib
import joblib
joblib.dump(models['RandomForest'],  'fraud_detection_model.pkl')

#Developing an API using Flask for serving predictions
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

model = joblib.load('fraud_detection_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
  data = request.json
  prediction = model.predict([data['features']])
  return jsonify({'prediction': int(prediction[0])})

if __name__ == '__main__':
  app.run(debug=True, port=5000)